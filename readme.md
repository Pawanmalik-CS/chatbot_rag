# ğŸ¤– RAG Chatbot

A Hybrid AI Chatbot built with Retrieval-Augmented Generation (RAG) architecture that answers questions based on your own documents and falls back to general knowledge when needed.

## ğŸ“Œ Project Overview

This chatbot can:
- Read and understand your own documents (TXT, PDF)
- Store and retrieve information using a Vector Database
- Answer questions based on your documents using Groq AI
- Fall back to general knowledge if the answer is not in your documents
- Auto-update the vector store when documents are added or modified

---

## ğŸ—ï¸ Architecture
```
User â†’ Streamlit UI â†’ FastAPI Backend â†’ RAG Pipeline â†’ ChromaDB + Groq LLM â†’ Response
```

### Three Layers:
- **Frontend** â€” Streamlit chat interface
- **Backend** â€” FastAPI server handling requests
- **RAG Pipeline** â€” Document loading, chunking, embedding, retrieval and generation

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | Streamlit |
| Backend | FastAPI + Uvicorn |
| RAG Orchestration | LangChain |
| Vector Database | ChromaDB |
| Embeddings | HuggingFace (all-MiniLM-L6-v2) |
| LLM | Groq (llama-3.1-8b-instant) |
| PDF Support | PyPDF |

---

## ğŸ“ Project Structure
```
chatbot_project/
â”œâ”€â”€ app.py              # Frontend - Streamlit UI
â”œâ”€â”€ main.py             # Backend - FastAPI server
â”œâ”€â”€ rag_pipeline.py     # RAG Pipeline - Core logic
â”œâ”€â”€ requirements.txt    # Project dependencies
â”œâ”€â”€ .env                # API keys (not committed)
â”œâ”€â”€ .gitignore          # Git ignore rules
â”œâ”€â”€ data/               # Your documents (TXT, PDF)
â”‚   â””â”€â”€ sample.txt
â””â”€â”€ chroma_db/          # Vector store (auto generated)
```

---

## âš™ï¸ Setup & Installation

### Prerequisites
- Python 3.11
- Groq API Key (get it from https://console.groq.com)

### Step 1 â€” Clone the repository
```bash
git clone https://github.com/Pawanmalik-CS/chatbot_rag.git
cd chatbot_rag
```

### Step 2 â€” Create virtual environment
```bash
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Mac/Linux
```

### Step 3 â€” Install dependencies
```bash
pip install -r requirements.txt
```

### Step 4 â€” Create `.env` file
```
GROQ_API_KEY=your_groq_api_key_here
```

### Step 5 â€” Add your documents
Add any `.txt` or `.pdf` files to the `data/` folder.

---

## ğŸš€ Running the Project

**Terminal 1 â€” Start Backend:**
```bash
uvicorn main:app --host 127.0.0.1 --port 8000
```

**Terminal 2 â€” Start Frontend:**
```bash
streamlit run app.py
```

Then open your browser at `http://localhost:8501`

---

## âœ¨ Features

- ğŸ¨ **Dark themed UI** with animated thinking dots and typing effect
- ğŸ’¬ **Chat history** displayed in sidebar
- ğŸ—‘ï¸ **Clear chat** button
- ğŸ“„ **Auto document ingestion** â€” just drop files in `data/` folder
- ğŸ”„ **Auto vector store rebuild** when documents change
- ğŸ¤– **Hybrid responses** â€” uses your docs first, falls back to general knowledge
- ğŸ›¡ï¸ **Safe responses** â€” declines harmful or sensitive topics

---

## ğŸ“¦ Dependencies

See [requirements.txt](https://github.com/Pawanmalik-CS/chatbot_rag/blob/master/requirements.txt)

---

## ğŸ”® Upcoming Features

- [ ] PDF upload from UI
- [ ] Multiple document collections
- [ ] Chat history persistence
- [ ] User authentication

---


##RAG Mode â€” When you ask something related to your documents in the data/ folder, it searches the vector database and answers from your documents
General Mode â€” When you ask something not in your documents (like "how are you" or "what is Python"), it falls back to Groq's general knowledge and answers normally

So it's hybrid because it combines both your custom data and general AI knowledge in one chatbot â€” giving the best of both worlds! ğŸš€

## ğŸ‘¨â€ğŸ’» Author

**Pawan Malik**
- GitHub: [@Pawanmalik-CS](https://github.com/Pawanmalik-CS)